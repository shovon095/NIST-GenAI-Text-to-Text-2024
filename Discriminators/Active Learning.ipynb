{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.5.1-cp312-cp312-win_amd64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\python312\\lib\\site-packages (from scikit-learn) (2.0.0)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Using cached scipy-1.14.0-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\python312\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Using cached scikit_learn-1.5.1-cp312-cp312-win_amd64.whl (10.9 MB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached scipy-1.14.0-cp312-cp312-win_amd64.whl (44.5 MB)\n",
      "Installing collected packages: scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.5.1 scipy-1.14.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/10\n",
      "Iteration 2/10\n",
      "Iteration 3/10\n",
      "Iteration 4/10\n",
      "Iteration 5/10\n",
      "Iteration 6/10\n",
      "Iteration 7/10\n",
      "Iteration 8/10\n",
      "Iteration 9/10\n",
      "Iteration 10/10\n",
      "Results saved.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, RobertaTokenizer, RobertaForSequenceClassification\n",
    "import torch\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Initial training data (manually labeled)\n",
    "initial_data = [\n",
    "    {\"text\": \"Bownes ideas had many predecessors, from Latin Christianity through Immanuel Kant, using many different theories and concepts, about what a human being is and about the personhood of God in its relation to our own personhood. His forceful argumentation influenced James, who helped found the American philosophical tradition of pragmatism shortly after Bowneâ€™s first books were published and who drew increasingly close to personalism, as did the idealist philosopher Josiah Royce. Bowne was at the centre of this troika of canonical American philosophers at the turn of the 20th century. His teaching rippled out through personalist philosophers on the West Coast and through his students at Boston, notably Edgar S Brightman and Harold DeWulf, both of whom later became teachers of King.\", \"label\": \"Human\"},\n",
    "    {\"text\": \"Spanish political parties mobilize against the ETA, planning a meeting to unite all democratic forces in their anti-terrorism efforts. This move comes amid a series of violent events involving the Basque separatist group ETA, which has been responsible for nearly 800 deaths since 1968 in its fight for an independent Basque state. Recent incidents include three non-fatal bombs in Malaga, the murders of a town councilor and a retired Civil Guard officer, and a fatal bombing targeting another town councilor. There are demonstrations demanding a negotiated solution to the violence, while Spain's Socialist party exits the Basque coalition government due to its connections with radical groups. Meanwhile, violent clashes with police and accusations from ETA against Spain and France of attempting to eliminate the Basque language highlight ongoing tensions. Elections in Spain's Basque region and a cease-fire announcement by ETA introduce a potential turn towards peace, supported internationally by figures like Gerry Adams, who urges US involvement in facilitating a resolution. However, Spain's government refuses to negotiate Basque independence in peace talks, considering compensation for victims of anti-Basque death squads from the 1980s and initiating steps towards peace talks with ETA. Basque separatists reaffirm their willingness to negotiate without fully committing to disarmament. The articles reflect a complex web of political maneuvers, violence, cultural struggle, and international attention surrounding the Basque separatist movement, marking a crucial phase in Spain's efforts to address regional unrest and move toward resolution.\", \"label\": \"AI\"}\n",
    "]\n",
    "\n",
    "# Load pre-trained BERT and RoBERTa models and tokenizers\n",
    "bert_model_name = \"bert-base-uncased\"\n",
    "roberta_model_name = \"roberta-base\"\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained(roberta_model_name)\n",
    "bert_model = BertForSequenceClassification.from_pretrained(bert_model_name)\n",
    "roberta_model = RobertaForSequenceClassification.from_pretrained(roberta_model_name)\n",
    "\n",
    "# Tokenize and encode the texts\n",
    "def encode_texts(texts, tokenizer):\n",
    "    return tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Prepare initial dataset\n",
    "def get_initial_dataset():\n",
    "    texts = [item[\"text\"] for item in initial_data]\n",
    "    labels = [1 if item[\"label\"] == \"AI\" else 0 for item in initial_data]\n",
    "    return texts, labels\n",
    "\n",
    "# Initial training of the models\n",
    "def initial_training():\n",
    "    texts, labels = get_initial_dataset()\n",
    "    encoded_texts_bert = encode_texts(texts, bert_tokenizer)\n",
    "    encoded_texts_roberta = encode_texts(texts, roberta_tokenizer)\n",
    "    y_train = torch.tensor(labels)\n",
    "\n",
    "    # Fine-tune BERT\n",
    "    bert_model.train()\n",
    "    optimizer_bert = torch.optim.Adam(bert_model.parameters(), lr=1e-5)\n",
    "    for epoch in range(3):  # Adjust the number of epochs as needed\n",
    "        optimizer_bert.zero_grad()\n",
    "        outputs_bert = bert_model(**encoded_texts_bert, labels=y_train)\n",
    "        loss_bert = outputs_bert.loss\n",
    "        loss_bert.backward()\n",
    "        optimizer_bert.step()\n",
    "    bert_model.eval()\n",
    "\n",
    "    # Fine-tune RoBERTa\n",
    "    roberta_model.train()\n",
    "    optimizer_roberta = torch.optim.Adam(roberta_model.parameters(), lr=1e-5)\n",
    "    for epoch in range(3):  # Adjust the number of epochs as needed\n",
    "        optimizer_roberta.zero_grad()\n",
    "        outputs_roberta = roberta_model(**encoded_texts_roberta, labels=y_train)\n",
    "        loss_roberta = outputs_roberta.loss\n",
    "        loss_roberta.backward()\n",
    "        optimizer_roberta.step()\n",
    "    roberta_model.eval()\n",
    "\n",
    "# Evaluate the models\n",
    "def evaluate_model(model, tokenizer, texts, labels):\n",
    "    encoded_texts = encode_texts(texts, tokenizer)\n",
    "    y_true = torch.tensor(labels)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoded_texts)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        accuracy = (preds == y_true).float().mean().item()\n",
    "    return accuracy\n",
    "\n",
    "# Active Learning Loop with BERT and RoBERTa\n",
    "def active_learning_loop_bert_roberta(input_directory, output_directory, results_file, iterations=10):\n",
    "    results = []\n",
    "    texts, labels = get_initial_dataset()\n",
    "    y_train = torch.tensor(labels)\n",
    "\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "        print(f\"Iteration {iteration+1}/{iterations}\")\n",
    "        new_texts = []\n",
    "        new_labels = []\n",
    "        high_confidence_samples = []\n",
    "\n",
    "        for filename in os.listdir(input_directory):\n",
    "            if filename.endswith(\".txt\"):\n",
    "                summary_path = os.path.join(input_directory, filename)\n",
    "                with open(summary_path, 'r', encoding='utf-8') as file:\n",
    "                    summary_content = file.read()\n",
    "                    \n",
    "                    # BERT predictions\n",
    "                    encoded_input_bert = encode_texts([summary_content], bert_tokenizer)\n",
    "                    with torch.no_grad():\n",
    "                        outputs_bert = bert_model(**encoded_input_bert)\n",
    "                        logits_bert = outputs_bert.logits\n",
    "                        probs_bert = torch.nn.functional.softmax(logits_bert, dim=-1)\n",
    "                        confidence_bert, predicted_class_bert = torch.max(probs_bert, dim=1)\n",
    "                    \n",
    "                    # RoBERTa predictions\n",
    "                    encoded_input_roberta = encode_texts([summary_content], roberta_tokenizer)\n",
    "                    with torch.no_grad():\n",
    "                        outputs_roberta = roberta_model(**encoded_input_roberta)\n",
    "                        logits_roberta = outputs_roberta.logits\n",
    "                        probs_roberta = torch.nn.functional.softmax(logits_roberta, dim=-1)\n",
    "                        confidence_roberta, predicted_class_roberta = torch.max(probs_roberta, dim=1)\n",
    "                    \n",
    "                    # Average confidence\n",
    "                    avg_confidence = (confidence_bert.item() + confidence_roberta.item()) / 2\n",
    "                    avg_predicted_class = 1 if (predicted_class_bert.item() + predicted_class_roberta.item()) / 2 >= 0.5 else 0\n",
    "\n",
    "                    if avg_confidence >= 0.7:  # Adjust confidence threshold as needed\n",
    "                        high_confidence_samples.append((summary_content, avg_predicted_class, avg_confidence))\n",
    "                    \n",
    "                    results.append([filename, \"AI\" if avg_predicted_class == 1 else \"Human\", avg_confidence])\n",
    "        \n",
    "        for sample in high_confidence_samples:\n",
    "            summary_content, avg_predicted_class, avg_confidence = sample\n",
    "            new_texts.append(summary_content)\n",
    "            new_labels.append(avg_predicted_class)\n",
    "        \n",
    "        if new_texts:\n",
    "            encoded_new_texts_bert = encode_texts(new_texts, bert_tokenizer)\n",
    "            encoded_new_texts_roberta = encode_texts(new_texts, roberta_tokenizer)\n",
    "            y_new = torch.tensor(new_labels)\n",
    "            texts.extend(new_texts)\n",
    "            y_train = torch.cat((y_train, y_new), dim=0)\n",
    "\n",
    "            # Fine-tune BERT\n",
    "            encoded_texts_bert = encode_texts(texts, bert_tokenizer)\n",
    "            bert_model.train()\n",
    "            optimizer_bert = torch.optim.Adam(bert_model.parameters(), lr=1e-5)\n",
    "            for epoch in range(3):  # Adjust the number of epochs as needed\n",
    "                optimizer_bert.zero_grad()\n",
    "                outputs_bert = bert_model(**encoded_texts_bert, labels=y_train)\n",
    "                loss_bert = outputs_bert.loss\n",
    "                loss_bert.backward()\n",
    "                optimizer_bert.step()\n",
    "            bert_model.eval()\n",
    "\n",
    "            # Fine-tune RoBERTa\n",
    "            encoded_texts_roberta = encode_texts(texts, roberta_tokenizer)\n",
    "            roberta_model.train()\n",
    "            optimizer_roberta = torch.optim.Adam(roberta_model.parameters(), lr=1e-5)\n",
    "            for epoch in range(3):  # Adjust the number of epochs as needed\n",
    "                optimizer_roberta.zero_grad()\n",
    "                outputs_roberta = roberta_model(**encoded_texts_roberta, labels=y_train)\n",
    "                loss_roberta = outputs_roberta.loss\n",
    "                loss_roberta.backward()\n",
    "                optimizer_roberta.step()\n",
    "            roberta_model.eval()\n",
    "\n",
    "            # Evaluate models\n",
    "            accuracy_bert = evaluate_model(bert_model, bert_tokenizer, texts, labels)\n",
    "            accuracy_roberta = evaluate_model(roberta_model, roberta_tokenizer, texts, labels)\n",
    "            print(f\"Iteration {iteration+1} - BERT Accuracy: {accuracy_bert:.4f}, RoBERTa Accuracy: {accuracy_roberta:.4f}\")\n",
    "\n",
    "    # Save results to CSV\n",
    "    with open(results_file, 'w', encoding='utf-8', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['Filename', 'Prediction', 'Confidence'])\n",
    "        writer.writerows(results)\n",
    "\n",
    "# Main execution\n",
    "input_directory = \"C:/Users/shouv/Desktop/Research/NIST/GenAI24-NIST-pilot-T2T-D-set-1/GenAI24-NIST-pilot-T2T-D-set-1/files/\"\n",
    "output_directory = \"C:/Users/shouv/Desktop/Research/NIST/GenAI24-NIST-pilot-T2T-D-set-1/GenAI24-NIST-pilot-T2T-D-set-1/results/\"\n",
    "results_file = \"C:/Users/shouv/Desktop/Research/NIST/GenAI24-NIST-pilot-T2T-D-set-1/GenAI24-NIST-pilot-T2T-D-set-1/results.csv\"\n",
    "\n",
    "# Initial training\n",
    "initial_training()\n",
    "\n",
    "# Active learning loop\n",
    "active_learning_loop_bert_roberta(input_directory, output_directory, results_file)\n",
    "\n",
    "print(\"Results saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
