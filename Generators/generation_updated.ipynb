{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1 for topic topic_5089: For example, The summarized text reports on two key topics involving the Southern Poverty Law Center (SPLC) and cultural reflections. The SPLC indicated a significant rise in organized hate groups in the U.S., identifying 474 such groups in 1997, a 20% increase from 1996. Moreover, this growth was driven by the emergence of new chapters of established groups, particularly the ku klux klan and christian identity churches, and was exacerbated by the rise of hate-promoting websites. Incidents of violence from neo-nazi skinheads were also noted. Moreover, in a separate piece, sinead o'connor discussed her past controversies, expressing pride in her actions against the pope while regretting some other protests. She also criticized younger artists for lacking controversy. For example, additionally, the splc faced backlash for designating the nation of islam as a hate group. The commentary also drew attention to the severity of recent racially motivated killings, such as the trial of John William King for the murder of James Byr\n",
      "Attempt 1 for topic topic_6609: In contrast, a recent eurobarometer survey shows that 54% of eu citizens support the euro, while 37% oppose it, though many feel uninformed about it. The European Commission has denied any plans to delay the euro's launch set for January 1, 1999, as outlined in the Maastricht Treaty. Additionally, french prime minister alain juppe reaffirmed france's commitment to the euro amid concerns over financial sovereignty. The european monetary institute is designing euro banknotes, aiming to represent european civilization. Moreover, UK lenders are preparing for euro mortgages, despite government skepticism. A zambian official noted potential benefits from the euro in trade. For example, the euro will have the code \"eur\" to aid in technical preparations before its launch.\n",
      "Attempt 1 for topic topic_9024: In contrast, additionally, **summary of basque separatism article highlights:** 1. In 1996, eta declared a week-long cease-fire that was dismissed by prime minister jose maria aznar as a propaganda tactic. Moreover, Additionally, the spanish government, led by interior minister javier mayor oreja, called for eta to release a hostage and emphasized a unified political response against the group. 2. Additionally, Moreover, following the cease-fire's conclusion, political parties in spain united in july 1996 to combat eta, excluding its political wing, herri batasuna. 3. In contrast, for example, eta conducted bombings in malaga in july 1996, which did not result in casualties, amidst a series of minor attacks. 4. Moreover, additionally, in june 1998, eta was responsible for two political murders, intensifying its violent efforts for basque independence. 5. For example, additionally, protests demanding peace negotiations erupted in 1998 after a local councilor's murder, highlighting a history of violence that has led to approximately 800 deaths since 1968. 6. Additionally, In contrast, Eff\n",
      "Attempt 1 for topic topic_4803: Additionally, **articles summary:** 1. **Action Films in Jakarta (06/08/1998)**: Jakarta theaters are featuring new action films, including *US Marshals* with Wesley Snipes, *Desperate Measures* starring Michael Keaton, *Playing God* with David Duchovny, and a reimagined *Great Expectations*. Moreover, The comedy *Money Talk$* continues to perform well. 2. Moreover, **Martin Margiela's Men's Line (09/14/1998)**: Martin Margiela will launch a men's-wear line named \"10\" in Paris on October 15, offering tailored and unisex designs to appeal to a wide audience alongside women's collections. 3. For example, **Golden Globe Winners Recap (01/24/1999)**: Gwyneth Paltrow and Michael Caine won acting awards at the Golden Globes, while Ed Harris and Lynn Redgrave won supporting categories. The ceremony featured heartfelt moments, including Michael J. Additionally, Fox addressing his Parkinson's disease. 4. It's interesting to note that **Articles Summary:** 1\n",
      "Attempt 1 for topic topic_3130: For example, pakistan asserts its nuclear capabilities are for defensive purposes and denies any plans for further nuclear tests, criticizing india's nuclear stance. International reactions to pakistan's nuclear tests include calls for adherence to non-proliferation treaties from countries like the u.s., nato, and japan, the latter urging pakistan to sign the comprehensive test ban treaty. Additionally, The Kashmir conflict is identified as a key factor heightening tensions between India and Pakistan, raising concerns from regional powers. Pakistan has unilaterally declared a testing moratorium while maintaining its nuclear deterrence right. Additionally, prime minister sharif denounced the nuclear non-proliferation treaty as discriminatory and emphasized the need for dialogue to mitigate security issues and prevent further testing.\n",
      "Attempt 1 for topic topic_5760: Let's dive into it, Additionally, **metallica and napster dispute summary** - **lawsuits:** metallica sued napster, three universities, and the riaa for copyright infringement, claiming unauthorized sharing of their music, seeking substantial damages. In response, the universities blocked access to napster. Additionally, - **Napster's Response:** Napster's CEO criticized Metallica for not reaching out before the lawsuit and stated they would close accounts of infringing users, but could not prevent new registrations. - **user backlash:** fans reacted negatively to metallica's legal actions, feeling betrayed, with some protesting by destroying their cds. For example, - **legal developments:** a judge ruled against napster, requiring them to stop sharing copyrighted material, which risked shutting down the service. - **Cultural Impact:** The dispute underscored a conflict between artists' rights and fans' desire for free music access, with some arguing Napster helped promote lesser-known artists. In contrast, - **Changes in University Policies:** Several universities reversed their initial support\n",
      "Attempt 1 for topic topic_4936: Let's dive into it, Additionally, vice president al gore formally announces his presidential campaign for the 2000 election, forming an organization and filing necessary paperwork. Other democratic candidates like bill bradley, john kerry, and dick gephardt are also expected to run. Moreover, gore's campaign is managed by craig smith, who plans to leverage previous campaign experience. Despite scrutiny from the clinton administration's controversies, gore has strong financial backing and political connections. In contrast, He appoints Jose Villarreal as treasurer to ensure compliance with campaign finance laws and reaches out to black lawmakers to strengthen voter mobilization. Additionally, gore faces pressure to denounce clinton's actions during impeachment while maintaining party support. For example, he receives endorsements from key party figures, suggesting solid democratic unity, and emphasizes personal voter interactions in new hampshire.. It's interesting to note that Vice President Al Gore formally announces his presidential campaign for the 2000 election, forming an organization and filing necessary paperwork\n",
      "Attempt 1 for topic topic_4745: For example, **compressed summary:** linda tripp, key in the clinton-lewinsky scandal, recorded conversations with monica lewinsky about her affair with president bill clinton, prompting an investigation by kenneth starr and leading to clinton's impeachment. Public opinion was divided, with around 48% believing Tripp overstepped her bounds while others felt the White House excessively leaked information. Moreover, tripp testified before a grand jury, claiming her intention was to protect lewinsky, though many viewed her actions as betrayal. Legally, she faced wiretapping charges for her recordings, but the case was dismissed due to lack of evidence. In contrast, Tripp attempted to rehabilitate her image through media, yet remained largely seen as a villain in the scandal, highlighting themes of loyalty, betrayal, and personal consequences in a political context.\n",
      "Attempt 1 for topic topic_3693: Additionally, **Library of Congress Updates:** 1. **China-US Library Conference (1996)**: About 100 librarians from China and the US convened to promote cooperation and discuss advancements in library science and technology. In contrast, 2. **research on libraries vs. For example, Internet (1997)**: A study affirmed that libraries provide more reliable information than the Internet, despite its prevalence. 3. In contrast, **Tibetan Book Collection (1997)**: The Library of Congress maintains a collection of 7,700 Tibetan works, primarily donated by diplomats, including texts from the 13th Dalai Lama. 4. Additionally, **Jefferson Letter Controversy (1998)**: A scholarly debate arose over Jefferson's 1802 letter about church-state separation, with differing interpretations on its implications for religion in government. 5. Additionally, **freud exhibit (1998)**: the largest exhibit on sigmund freud was held at the library of congress, amidst discussions on how to present his controversial legacy. 6. Additionally, **digital information preservation\n",
      "Attempt 1 for topic topic_5246: To put it simply, Additionally, O.J. Simpson was found liable for the wrongful deaths of his ex-wife nicole brown simpson and her friend ronald goldman, resulting in a $8.5 million compensatory damages award. Moreover, This followed a 1995 criminal trial where he was acquitted of murder. His house was sold at auction for $2.63 million after the civil judgment. Additionally, simpson is appealing the total $33.5 million judgment, which has drawn ire from the victims' families who accuse him of evading responsibility. Additionally, the IRS is claiming $6.5 million from Nicole's estate due to Simpson's unpaid judgment. In contrast, An auction of Simpson's memorabilia is planned, although bidders have mixed feelings, especially after a notable item, the Heisman Trophy, had a misspelled plaque.\n",
      "Summaries and GPT-3.5 results saved.\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "import random\n",
    "import csv\n",
    "import xml.etree.ElementTree as ET\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up your OpenAI API key\n",
    "openai.api_key = ''\n",
    "MAX_TOKENS_PER_REQUEST = 2000  # Max tokens per request to avoid exceeding the context length\n",
    "MAX_WORDS_FINAL_SUMMARY = 250  # Maximum words for the final summary\n",
    "\n",
    "# Placeholder for user profiles\n",
    "user_profiles = {\n",
    "    \"user1\": {\"tone\": \"optimistic\", \"style\": \"informal\", \"focus\": \"narrative\"},\n",
    "    \"user2\": {\"tone\": \"neutral\", \"style\": \"formal\", \"focus\": \"technical\"}\n",
    "}\n",
    "\n",
    "def estimate_tokens(text):\n",
    "    \"\"\"\n",
    "    Estimate the number of tokens for the given text.\n",
    "    \"\"\"\n",
    "    return len(text.split())\n",
    "\n",
    "def chunk_text(text, max_tokens=MAX_TOKENS_PER_REQUEST):\n",
    "    \"\"\"\n",
    "    Splits the text into smaller chunks that fit within the token limit.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    \n",
    "    for word in words:\n",
    "        word_length = len(word.split())\n",
    "        if current_length + word_length + 1 > max_tokens:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "        current_chunk.append(word)\n",
    "        current_length += word_length + 1\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def chunk_text_by_paragraphs(text, max_tokens=MAX_TOKENS_PER_REQUEST):\n",
    "    \"\"\"\n",
    "    Splits the text into smaller chunks by paragraph that fit within the token limit.\n",
    "    \"\"\"\n",
    "    paragraphs = text.split(\"\\n\\n\")\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    \n",
    "    for paragraph in paragraphs:\n",
    "        paragraph_length = estimate_tokens(paragraph)\n",
    "        if current_length + paragraph_length > max_tokens:\n",
    "            chunks.append(\"\\n\\n\".join(current_chunk))\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "        current_chunk.append(paragraph)\n",
    "        current_length += paragraph_length\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(\"\\n\\n\".join(current_chunk))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def analyze_context(text):\n",
    "    \"\"\"\n",
    "    Analyze the context of the given text to determine its type.\n",
    "    \"\"\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an analyzer that determines the context of the following text.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Please analyze and describe the context of this text:\\n\\n{text}\"}\n",
    "        ],\n",
    "        max_tokens=100\n",
    "    )\n",
    "    return response.choices[0].message['content'].strip()\n",
    "\n",
    "def context_aware_summarization(text, engine=\"gpt-4o-mini\"):\n",
    "    \"\"\"\n",
    "    Summarize text with context-awareness based on the content's nature.\n",
    "    \"\"\"\n",
    "    context = analyze_context(text)\n",
    "    if \"technical\" in context.lower():\n",
    "        return summarize_text(text, engine, prompt_type=\"technical\")\n",
    "    elif \"narrative\" in context.lower():\n",
    "        return summarize_text(text, engine, prompt_type=\"narrative\")\n",
    "    else:\n",
    "        return summarize_text(text, engine)\n",
    "\n",
    "def summarize_text(text, engine=\"gpt-4o-mini\", prompt_type=\"default\"):\n",
    "    \"\"\"\n",
    "    Use OpenAI GPT-4 to summarize a given text chunk with customizable prompts.\n",
    "    \"\"\"\n",
    "    if prompt_type == \"technical\":\n",
    "        system_message = \"You are a concise and precise assistant.\"\n",
    "    elif prompt_type == \"narrative\":\n",
    "        system_message = \"You are a creative and engaging assistant.\"\n",
    "    else:\n",
    "        system_message = \"You are a helpful assistant.\"\n",
    "    \n",
    "    # Ensure text is within token limit\n",
    "    if estimate_tokens(text) > MAX_TOKENS_PER_REQUEST:\n",
    "        chunks = chunk_text(text, MAX_TOKENS_PER_REQUEST)\n",
    "        text = chunks[0]  # Use the first chunk for summarization\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=engine,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": f\"Summarize the following text:\\n\\n{text}\"}\n",
    "        ],\n",
    "        max_tokens=200\n",
    "    )\n",
    "    return response.choices[0].message['content'].strip()\n",
    "\n",
    "def add_human_like_features(summary):\n",
    "    \"\"\"\n",
    "    Modify the summary to include more human-like features, including varied sentence structures and tone adjustments.\n",
    "    \"\"\"\n",
    "    # Introduce transitions between sentences\n",
    "    transitions = [\"Moreover,\", \"Additionally,\", \"In contrast,\", \"For example,\"]\n",
    "    sentences = summary.split(\". \")\n",
    "    enhanced_summary = \". \".join([random.choice(transitions) + \" \" + sentence if i % 2 == 0 else sentence for i, sentence in enumerate(sentences)])\n",
    "    \n",
    "    # Vary sentence length and structure\n",
    "    enhanced_summary = \". \".join([sentence.capitalize() if random.random() < 0.5 else sentence for sentence in enhanced_summary.split(\". \")])\n",
    "\n",
    "    # Introduce minor informal elements or conversational tones\n",
    "    conversational_phrases = [\"Let's dive into it,\", \"So, basically,\", \"To put it simply,\"]\n",
    "    if random.random() < 0.2:\n",
    "        enhanced_summary = random.choice(conversational_phrases) + \" \" + enhanced_summary\n",
    "\n",
    "    # Add more subjective or emotional language carefully\n",
    "    emotional_phrases = [\"This is fascinating because\", \"Surprisingly,\", \"It's interesting to note that\"]\n",
    "    if random.random() < 0.2:\n",
    "        enhanced_summary = enhanced_summary + \". \" + random.choice(emotional_phrases) + \" \" + sentences[0]\n",
    "\n",
    "    return enhanced_summary\n",
    "\n",
    "def enhance_and_trim_summary(summary, max_words):\n",
    "    \"\"\"\n",
    "    Enhance the summary with human-like features and ensure it does not exceed the word limit.\n",
    "    \"\"\"\n",
    "    enhanced_summary = add_human_like_features(summary)\n",
    "    \n",
    "    return enhanced_summary\n",
    "\n",
    "def explicit_summarize(summary, engine=\"gpt-4o-mini\", max_words=MAX_WORDS_FINAL_SUMMARY):\n",
    "    \"\"\"\n",
    "    Explicitly ask GPT to summarize the enhanced summary to a specific word count.\n",
    "    \"\"\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=engine,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Summarize the following text to {max_words} words:\\n\\n{summary}\"}\n",
    "        ],\n",
    "        max_tokens=max_words\n",
    "    )\n",
    "    final_summary = response.choices[0].message['content'].strip()\n",
    "\n",
    "    # Ensure the final summary ends properly\n",
    "    if not final_summary.endswith('.'):\n",
    "        final_summary = final_summary.rsplit(' ', 1)[0] + '.'\n",
    "    \n",
    "    return final_summary\n",
    "\n",
    "def remove_special_characters(summary):\n",
    "    \"\"\"\n",
    "    Remove unnecessary special characters from the summary.\n",
    "    \"\"\"\n",
    "    # Remove unnecessary special characters but keep those that aid readability\n",
    "    summary = summary.replace(';', ',')\n",
    "    summary = summary.replace('+', 'and')\n",
    "    \n",
    "    # Avoid over-sanitization\n",
    "    summary = summary.replace('...', '.').replace('!!', '!').replace('??', '?')\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def check_with_simulated_personas(summary, personas=None):\n",
    "    \"\"\"\n",
    "    Use multiple personas to check if the summary is detected as AI-generated or human-written.\n",
    "    \"\"\"\n",
    "    if not personas:\n",
    "        personas = [\"AI Expert\", \"Casual Reader\", \"Technical Reviewer\"]\n",
    "\n",
    "    results = []\n",
    "    for persona in personas:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": f\"You are a {persona}. Detect if the following summary is AI-generated or human-written.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Is this summary AI-generated? Answer 'AI' or 'Human':\\n\\n{summary}\"}\n",
    "            ]\n",
    "        )\n",
    "        results.append(response.choices[0].message['content'].strip())\n",
    "\n",
    "    return max(set(results), key=results.count)  # Return the most common result\n",
    "\n",
    "def iterative_summary_refinement(summary, max_iterations=3):\n",
    "    \"\"\"\n",
    "    Iteratively refine the summary to make it more human-like.\n",
    "    \"\"\"\n",
    "    for _ in range(max_iterations):\n",
    "        detection_result = check_with_simulated_personas(summary)\n",
    "        if detection_result == \"Human\":\n",
    "            break\n",
    "        summary = enhance_and_trim_summary(summary, MAX_WORDS_FINAL_SUMMARY)\n",
    "    return summary\n",
    "\n",
    "def customize_summary(summary, detail_level=\"concise\", focus_area=\"general\"):\n",
    "    \"\"\"\n",
    "    Customize the summary based on user preferences for detail level and focus area.\n",
    "    \"\"\"\n",
    "    if detail_level == \"detailed\":\n",
    "        summary += \" Further analysis reveals deeper insights into the matter.\"\n",
    "    elif detail_level == \"concise\":\n",
    "        summary = \"In summary: \" + summary.split('.')[0] + \".\"\n",
    "\n",
    "    if focus_area == \"technical\":\n",
    "        summary += \" The technical aspects are explored extensively.\"\n",
    "    elif focus_area == \"narrative\":\n",
    "        summary = \"Storytelling elements make the topic engaging: \" + summary\n",
    "\n",
    "    return summary\n",
    "\n",
    "def blend_summaries(summaries):\n",
    "    \"\"\"\n",
    "    Blend multiple summaries to create a more diverse final output.\n",
    "    \"\"\"\n",
    "    blended_summary = []\n",
    "    for i in range(max(len(s.split()) for s in summaries)):\n",
    "        for summary in summaries:\n",
    "            words = summary.split()\n",
    "            if i < len(words):\n",
    "                blended_summary.append(words[i])\n",
    "    return \" \".join(blended_summary)\n",
    "\n",
    "def semantic_compression(text):\n",
    "    \"\"\"\n",
    "    Compress the text semantically by removing redundant information while maintaining key points.\n",
    "    \"\"\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a text compressor.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Compress the following text by removing redundant information while maintaining the key points:\\n\\n{text}\"}\n",
    "        ],\n",
    "        max_tokens=300\n",
    "    )\n",
    "    return response.choices[0].message['content'].strip()\n",
    "\n",
    "def adjust_tone_and_style(summary, tone=\"neutral\", style=\"formal\"):\n",
    "    \"\"\"\n",
    "    Adjust the tone and style of the summary based on user preferences.\n",
    "    \"\"\"\n",
    "    if tone == \"optimistic\":\n",
    "        summary = summary.replace(\"is\", \"is positively\").replace(\"are\", \"are positively\")\n",
    "        summary = \"Exciting news! \" + summary\n",
    "    elif tone == \"pessimistic\":\n",
    "        summary = summary.replace(\"is\", \"is unfortunately\").replace(\"are\", \"are unfortunately\")\n",
    "        summary = \"Sadly, \" + summary\n",
    "\n",
    "    if style == \"informal\":\n",
    "        summary = summary.replace(\"do not\", \"don't\").replace(\"cannot\", \"can't\")\n",
    "        summary = \"Hey there, \" + summary\n",
    "    elif style == \"formal\":\n",
    "        summary = summary.replace(\"don't\", \"do not\").replace(\"can't\", \"cannot\")\n",
    "        summary = \"Please note that \" + summary\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "def personalize_summary(summary, user_id):\n",
    "    \"\"\"\n",
    "    Personalize the summary based on user profiles.\n",
    "    \"\"\"\n",
    "    profile = user_profiles.get(user_id, {})\n",
    "    if profile.get(\"tone\"):\n",
    "        summary = adjust_tone_and_style(summary, tone=profile[\"tone\"], style=profile.get(\"style\", \"formal\"))\n",
    "    if profile.get(\"focus\"):\n",
    "        summary = customize_summary(summary, focus_area=profile[\"focus\"])\n",
    "    return summary\n",
    "\n",
    "def summarize_articles(article_contents, engine=\"gpt-4o-mini\"):\n",
    "    \"\"\"\n",
    "    Use OpenAI GPT-4 to summarize multiple articles with added human-like features.\n",
    "    \"\"\"\n",
    "    combined_content = \"\\n\\n\".join(article_contents)\n",
    "    combined_content = semantic_compression(combined_content)\n",
    "    \n",
    "    # Chunk the content to fit within the token limit\n",
    "    if estimate_tokens(combined_content) > MAX_TOKENS_PER_REQUEST:\n",
    "        chunks = chunk_text_by_paragraphs(combined_content)\n",
    "    else:\n",
    "        chunks = [combined_content]\n",
    "    \n",
    "    summaries = []\n",
    "    for chunk in chunks:\n",
    "        summary = context_aware_summarization(chunk, engine)\n",
    "        summaries.append(summary)\n",
    "    \n",
    "    combined_summary = blend_summaries(summaries)\n",
    "    \n",
    "    # Summarize the combined summary to ensure it is within the word limit\n",
    "    final_summary = iterative_summary_refinement(combined_summary)\n",
    "    \n",
    "    return final_summary\n",
    "\n",
    "def visualize_summary(summary):\n",
    "    \"\"\"\n",
    "    Generate a visual representation of the summary.\n",
    "    \"\"\"\n",
    "    sentences = summary.split('. ')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.barh(range(len(sentences)), [len(sentence) for sentence in sentences], align='center')\n",
    "    ax.set_yticks(range(len(sentences)))\n",
    "    ax.set_yticklabels(sentences)\n",
    "    ax.invert_yaxis()  # labels read top-to-bottom\n",
    "    ax.set_xlabel('Length of Sentence')\n",
    "    ax.set_title('Visual Representation of Summary')\n",
    "    plt.show()\n",
    "\n",
    "def collect_user_feedback(summary, user_feedback):\n",
    "    \"\"\"\n",
    "    Collect and log user feedback for continuous improvement.\n",
    "    \"\"\"\n",
    "    with open('feedback_log.csv', 'a') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([summary, user_feedback])\n",
    "    # Use this data to fine-tune future summarizations\n",
    "\n",
    "def batch_summarize(articles, engine=\"gpt-4o-mini\"):\n",
    "    \"\"\"\n",
    "    Batch process articles for summarization.\n",
    "    \"\"\"\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        summaries = list(executor.map(lambda article: summarize_text(article, engine), articles))\n",
    "    return summaries\n",
    "\n",
    "def summarize_with_retries(text, max_retries=3):\n",
    "    \"\"\"\n",
    "    Summarize text with retries in case of errors.\n",
    "    \"\"\"\n",
    "    attempts = 0\n",
    "    while attempts < max_retries:\n",
    "        try:\n",
    "            return summarize_text(text)\n",
    "        except Exception as e:\n",
    "            attempts += 1\n",
    "            with open('error_log.txt', 'a') as log:\n",
    "                log.write(f\"Error during summarization attempt {attempts} for text chunk: {text[:200]}... Error: {e}\\n\")\n",
    "            print(f\"Error summarizing text: {e}. Attempt {attempts}/{max_retries}\")\n",
    "    return \"Failed to generate a summary after multiple attempts.\"\n",
    "\n",
    "def parse_topics_file(topics_file):\n",
    "    \"\"\"\n",
    "    Parse the topics file to map articles to topics.\n",
    "    \"\"\"\n",
    "    with open(topics_file, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    # Wrap the content in a single root element\n",
    "    wrapped_content = f\"<root>\\n{content}\\n</root>\"\n",
    "    \n",
    "    tree = ET.ElementTree(ET.fromstring(wrapped_content))\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    topics = {}\n",
    "    for topic in root.findall('topic'):\n",
    "        topic_id = topic.find('num').text.strip()\n",
    "        articles = topic.find('docs').text.split()\n",
    "        topics[topic_id] = articles\n",
    "    \n",
    "    return topics\n",
    "\n",
    "def process_and_save_summaries(input_directory, output_directory, results_file, topics_file):\n",
    "    \"\"\"\n",
    "    Process all articles by topics, generate summaries with human-like features,\n",
    "    save summaries in another directory, and save GPT-3.5 results to a CSV file.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    root_element = ET.Element(\"GeneratorResults\", teamName=\"PV-Credit\")\n",
    "\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    topics = parse_topics_file(topics_file)\n",
    "\n",
    "    for topic_id, article_filenames in topics.items():\n",
    "        article_contents = []\n",
    "\n",
    "        for filename in article_filenames:\n",
    "            article_path = os.path.join(input_directory, filename)\n",
    "            if os.path.exists(article_path):\n",
    "                with open(article_path, 'r', encoding='utf-8') as file:\n",
    "                    article_contents.append(file.read())\n",
    "            else:\n",
    "                print(f\"File not found: {article_path}\")\n",
    "\n",
    "        if not article_contents:\n",
    "            print(f\"No articles found for topic {topic_id}.\")\n",
    "            continue\n",
    "\n",
    "        attempts = 0\n",
    "        human_like_summary = \"\"\n",
    "        detection_result = \"AI\"  # Initialize with a default value\n",
    "\n",
    "        while attempts < 3:\n",
    "            attempts += 1\n",
    "            try:\n",
    "                summary = summarize_articles(article_contents)\n",
    "                human_like_summary = enhance_and_trim_summary(summary, MAX_WORDS_FINAL_SUMMARY)\n",
    "                human_like_summary = remove_special_characters(human_like_summary)\n",
    "                \n",
    "                # Print attempt number and generated summary\n",
    "                print(f\"Attempt {attempts} for topic {topic_id}: {human_like_summary}\")\n",
    "\n",
    "                # Check summary with simulated personas\n",
    "                detection_result = check_with_simulated_personas(human_like_summary)\n",
    "\n",
    "                if detection_result == \"Human\":\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(f\"Error during summarization attempt {attempts} for topic {topic_id}: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Explicitly summarize to the final word limit\n",
    "        human_like_summary = explicit_summarize(human_like_summary)\n",
    "\n",
    "        results.append([topic_id, detection_result])\n",
    "\n",
    "        # Add summary to XML structure\n",
    "        run_result_element = ET.SubElement(root_element, \"GeneratorRunResult\", trainingData=\"No training data used\", version=\"1.0\", priority=\"1\", trained=\"F\", desc=\"Generated summary\", link=\"https://github.com/PV-Credit/ModelRepo\")\n",
    "        topic_result_element = ET.SubElement(run_result_element, \"GeneratorTopicResult\", topic=topic_id, elapsedTime=\"5\")\n",
    "        topic_result_element.text = human_like_summary\n",
    "\n",
    "    # Save XML\n",
    "    tree = ET.ElementTree(root_element)\n",
    "    xml_output_path = os.path.join(output_directory, \"summaries.xml\")\n",
    "    tree.write(xml_output_path, encoding='ISO-8859-1', xml_declaration=True)\n",
    "\n",
    "    # Save GPT-3.5 results to CSV\n",
    "    with open(results_file, 'w', encoding='utf-8', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['TopicID', 'Detection'])\n",
    "        writer.writerows(results)\n",
    "\n",
    "# Main execution\n",
    "input_directory = \"C:/Users/shouv/Desktop/Research/NIST/GenAI24-NIST-pilot-T2T-G-set-1/GenAI24-NIST-pilot-T2T-G-set-1/files/\"\n",
    "output_directory = 'C:/Users/shouv/Desktop/Research/NIST/GenAI24-NIST-pilot-T2T-G-set-1/GenAI24-NIST-pilot-T2T-G-set-1/Summaries/'\n",
    "results_file = 'C:/Users/shouv/Desktop/Research/NIST/GenAI24-NIST-pilot-T2T-G-set-1/GenAI24-NIST-pilot-T2T-G-set-1/gpt_35_results.csv'\n",
    "topics_file = 'C:/Users/shouv/Desktop/Research/NIST/GenAI24-NIST-pilot-T2T-G-set-1/GenAI24-NIST-pilot-T2T-G-set-1/GenAI24-NIST-pilot-T2T-G-set-1_topics.sgml'\n",
    "\n",
    "process_and_save_summaries(input_directory, output_directory, results_file, topics_file)\n",
    "\n",
    "print(\"Summaries and GPT-3.5 results saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
